{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1863ef-4d89-4a84-b4b2-f7a715dd07e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import joblib\n",
    "import traceback\n",
    "from io import BytesIO\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix,\n",
    "                             roc_curve, auc, precision_recall_fscore_support)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "\n",
    "sns.set()\n",
    "st.set_page_config(page_title=\"Electricity Consumption â€” Predictive Analytics (Project V2)\", layout=\"wide\")\n",
    "\n",
    "# -------------------------\n",
    "# Helpers & compatibility\n",
    "# -------------------------\n",
    "def make_onehot():\n",
    "    try:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "    except TypeError:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "    except Exception:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "@st.cache_data\n",
    "def load_csv(file):\n",
    "    return pd.read_csv(file)\n",
    "\n",
    "# Wordcloud helpers (build corpus from many columns + create image)\n",
    "def build_full_corpus(df, cols=None, numeric_to_bins=True, bins=5, use_unique_for=None):\n",
    "    \"\"\"\n",
    "    Build a corpus string from dataframe columns.\n",
    "      - cols: list of columns to include (None => all columns)\n",
    "      - numeric_to_bins: convert numeric columns to binned labels (True recommended)\n",
    "      - bins: number of quantile bins for numeric columns\n",
    "      - use_unique_for: list of columns to use only unique values (avoid domination by repeated token)\n",
    "    \"\"\"\n",
    "    if cols is None:\n",
    "        cols = df.columns.tolist()\n",
    "    if use_unique_for is None:\n",
    "        use_unique_for = []\n",
    "\n",
    "    parts = []\n",
    "    for c in cols:\n",
    "        if c not in df.columns:\n",
    "            continue\n",
    "        ser = df[c].dropna()\n",
    "        if ser.empty:\n",
    "            continue\n",
    "\n",
    "        # numeric -> bins\n",
    "        if pd.api.types.is_numeric_dtype(df[c]) and numeric_to_bins:\n",
    "            try:\n",
    "                b = pd.qcut(ser, q=bins, duplicates='drop').astype(str)\n",
    "                parts.append(\" \".join([f\"{c}_{x}\" for x in b.tolist()]))\n",
    "            except Exception:\n",
    "                parts.append(\" \".join(ser.astype(str).tolist()))\n",
    "        else:\n",
    "            if c in use_unique_for:\n",
    "                vals = ser.astype(str).unique().tolist()\n",
    "            else:\n",
    "                vals = ser.astype(str).tolist()\n",
    "            parts.append(\" \".join(vals))\n",
    "\n",
    "    corpus = \" \".join(parts)\n",
    "    # limit very long corpus to avoid memory/wordcloud issues\n",
    "    max_chars = 800000\n",
    "    if len(corpus) > max_chars:\n",
    "        corpus = corpus[:max_chars]\n",
    "    return corpus\n",
    "\n",
    "def make_wordcloud_from_corpus(corpus, width=1200, height=480, max_words=350, background_color=\"white\", colormap='tab20'):\n",
    "    \"\"\"Return a PIL image of a WordCloud built from corpus string (or None if empty).\"\"\"\n",
    "    if not corpus or str(corpus).strip() == \"\":\n",
    "        return None\n",
    "    stop = set(STOPWORDS)\n",
    "    wc = WordCloud(width=width, height=height, background_color=background_color,\n",
    "                   collocations=False, stopwords=stop, max_words=max_words, colormap=colormap)\n",
    "    img = wc.generate(corpus)\n",
    "    return img.to_image()\n",
    "\n",
    "def safe_numeric_series(s):\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "def download_bytesio(obj):\n",
    "    bio = BytesIO()\n",
    "    joblib.dump(obj, bio)\n",
    "    bio.seek(0)\n",
    "    return bio\n",
    "\n",
    "# -------------------------\n",
    "# Sidebar: upload & settings\n",
    "# -------------------------\n",
    "st.sidebar.title(\"Data & Settings\")\n",
    "uploaded = st.sidebar.file_uploader(\"Upload CSV (TG-SPDCL dataset)\", type=[\"csv\"])\n",
    "sample_mode = st.sidebar.checkbox(\"Train & EDA on sample (faster)\", value=True)\n",
    "sample_frac = st.sidebar.slider(\"Sampling fraction (for training/EDA)\", 0.05, 1.0, 0.2, step=0.05) if sample_mode else 1.0\n",
    "show_trace = st.sidebar.checkbox(\"Show tracebacks on error\", value=False)\n",
    "train_full = st.sidebar.checkbox(\"Force full-data training\", value=False)\n",
    "\n",
    "if not uploaded:\n",
    "    st.info(\"Upload your CSV to proceed.\")\n",
    "    st.stop()\n",
    "\n",
    "# -------------------------\n",
    "# Load dataframe\n",
    "# -------------------------\n",
    "df = load_csv(uploaded)\n",
    "\n",
    "# -------------------------\n",
    "# Title & Project header\n",
    "# -------------------------\n",
    "st.markdown(\n",
    "    \"<h1 style='color:#0f4c81; margin:0'>Electricity Consumption â€” Predictive Analytics (Project)</h1>\"\n",
    "    \"<p style='color:#6b7280; margin-top:6px'>Supervised: Logistic / KNN / DecisionTree Â· Unsupervised: KMeans / Agglomerative Â· EDA & Exports</p>\",\n",
    "    unsafe_allow_html=True,\n",
    ")\n",
    "st.write(\"Dataset shape:\", df.shape)\n",
    "\n",
    "# Expected columns (defaults from your notebook)\n",
    "NUM_COLS = ['totservices', 'billdservices', 'units', 'load']\n",
    "CAT_COLS = ['circle', 'division', 'subdivision', 'section', 'area', 'catdesc', 'catcode']\n",
    "\n",
    "# -------------------------\n",
    "# Tabs UI\n",
    "# -------------------------\n",
    "tabs = st.tabs([\"ðŸ“„ Data\", \"ðŸ” EDA\", \"âš™ Preprocessing\", \"ðŸ¤– Supervised\", \"ðŸ“Š Model Comparison\", \"ðŸ”® Predict (auto)\", \"ðŸŒ€ Unsupervised\", \"ðŸ’¾ Save Models\"])\n",
    "\n",
    "# -------------------------\n",
    "# TAB: Data\n",
    "# -------------------------\n",
    "with tabs[0]:\n",
    "    st.header(\"Data Preview\")\n",
    "    st.write(\"Shape:\", df.shape)\n",
    "    st.dataframe(df.head(200))\n",
    "    st.write(\"Columns detected:\", list(df.columns))\n",
    "\n",
    "# -------------------------\n",
    "# TAB: EDA\n",
    "# -------------------------\n",
    "with tabs[1]:\n",
    "    st.header(\"Exploratory Data Analysis\")\n",
    "\n",
    "    # WordCloud controls (all/selected cols)\n",
    "    st.subheader(\"WordCloud â€” choose columns (All columns recommended)\")\n",
    "    all_cols = list(df.columns)\n",
    "    default_cols = ['catdesc'] if 'catdesc' in df.columns else all_cols[:3]\n",
    "    cols_choice = st.multiselect(\"Pick columns to include in WordCloud (leave empty to use ALL)\", options=all_cols, default=default_cols)\n",
    "    numeric_to_bins = st.checkbox(\"Convert numeric columns to binned labels (recommended)\", value=True)\n",
    "    use_unique = st.multiselect(\"Treat these columns by UNIQUE values only (avoid repetition dominating)\", options=all_cols, default=['catdesc'] if 'catdesc' in df.columns else [])\n",
    "\n",
    "    if st.button(\"Generate WordCloud\"):\n",
    "        with st.spinner(\"Building wordcloud â€” this may take a few seconds...\"):\n",
    "            cols_to_use = cols_choice if len(cols_choice) > 0 else None\n",
    "            corpus = build_full_corpus(df, cols=cols_to_use, numeric_to_bins=numeric_to_bins, bins=5, use_unique_for=use_unique)\n",
    "            img = make_wordcloud_from_corpus(corpus, width=1300, height=480, max_words=400, colormap='tab20')\n",
    "            if img is None:\n",
    "                st.info(\"No text found for the selected columns â€” try different columns.\")\n",
    "            else:\n",
    "                st.image(img, use_container_width=True)\n",
    "\n",
    "    # Numeric histograms\n",
    "    st.subheader(\"Numeric Histograms\")\n",
    "    num_present = [c for c in NUM_COLS if c in df.columns]\n",
    "    if num_present:\n",
    "        cols = st.columns(min(4, len(num_present)))\n",
    "        for i, c in enumerate(num_present):\n",
    "            fig = px.histogram(df, x=c, nbins=40, title=c)\n",
    "            cols[i % len(cols)].plotly_chart(fig, use_container_width=True)\n",
    "    else:\n",
    "        st.info(\"No numeric columns found for histograms.\")\n",
    "\n",
    "    # Boxplots\n",
    "    st.subheader(\"Boxplots\")\n",
    "    if num_present:\n",
    "        cols = st.columns(min(4, len(num_present)))\n",
    "        for i, c in enumerate(num_present):\n",
    "            fig, ax = plt.subplots(figsize=(6,2))\n",
    "            sns.boxplot(x=safe_numeric_series(df[c]).dropna(), ax=ax, color=\"#f6a821\")\n",
    "            ax.set_title(c)\n",
    "            cols[i % len(cols)].pyplot(fig)\n",
    "\n",
    "    # Countplot for chosen categorical\n",
    "    st.subheader(\"Countplot (categorical)\")\n",
    "    cats_present = [c for c in df.columns if df[c].dtype == object][:40]\n",
    "    sel_cat = st.selectbox(\"Choose categorical column\", options=[None] + cats_present)\n",
    "    if sel_cat:\n",
    "        vc = df[sel_cat].astype(str).value_counts().nlargest(25)\n",
    "        fig, ax = plt.subplots(figsize=(8,4))\n",
    "        sns.barplot(x=vc.values, y=vc.index, ax=ax)\n",
    "        ax.set_title(f\"Top values for {sel_cat}\")\n",
    "        st.pyplot(fig)\n",
    "    else:\n",
    "        st.info(\"Choose a categorical column to see counts.\")\n",
    "\n",
    "    # Correlation heatmap\n",
    "    st.subheader(\"Correlation heatmap (numeric)\")\n",
    "    if len(num_present) >= 2:\n",
    "        fig, ax = plt.subplots(figsize=(8,5))\n",
    "        sns.heatmap(df[num_present].apply(pd.to_numeric, errors='coerce').corr(), annot=True, cmap=\"coolwarm\", ax=ax)\n",
    "        st.pyplot(fig)\n",
    "    else:\n",
    "        st.info(\"Not enough numeric columns for correlation heatmap.\")\n",
    "\n",
    "    # Scatter matrix (plotly sample)\n",
    "    if len(num_present) >= 2:\n",
    "        st.subheader(\"Scatter matrix (sampled)\")\n",
    "        cols_for_matrix = num_present[:6]\n",
    "        sample_n = int(min(2000, max(100, len(df) * (sample_frac if sample_mode else 1.0))))\n",
    "        samp = df[cols_for_matrix].sample(n=sample_n, random_state=42) if len(df) > sample_n else df[cols_for_matrix]\n",
    "        samp = samp.dropna()\n",
    "        if samp.shape[0] > 2:\n",
    "            fig = px.scatter_matrix(samp, dimensions=cols_for_matrix, height=600)\n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "        else:\n",
    "            st.info(\"Not enough sample rows for scatter matrix.\")\n",
    "    else:\n",
    "        st.info(\"Need at least 2 numeric columns for scatter matrix.\")\n",
    "\n",
    "# -------------------------\n",
    "# TAB: Preprocessing\n",
    "# -------------------------\n",
    "with tabs[2]:\n",
    "    st.header(\"Preprocessing Setup & Preview (safe)\")\n",
    "\n",
    "    num_present = [c for c in NUM_COLS if c in df.columns]\n",
    "    cat_present = [c for c in CAT_COLS if c in df.columns]\n",
    "\n",
    "    st.write(\"Numeric columns detected:\", num_present)\n",
    "    st.write(\"Categorical columns detected (expected):\", cat_present)\n",
    "\n",
    "    transformers = []\n",
    "    if num_present:\n",
    "        transformers.append(('num', Pipeline([('impute', SimpleImputer(strategy='median')), ('scale', StandardScaler())]), num_present))\n",
    "    if cat_present:\n",
    "        transformers.append(('cat', Pipeline([('impute', SimpleImputer(strategy='most_frequent')), ('ohe', make_onehot())]), cat_present))\n",
    "\n",
    "    if not transformers:\n",
    "        st.info(\"No transformers can be created: ensure your dataset includes expected numeric/categorical columns.\")\n",
    "    else:\n",
    "        preprocess = ColumnTransformer(transformers)\n",
    "        st.success(\"Preprocessing pipeline preview created.\")\n",
    "\n",
    "        # SAFE preview transform: coerce numeric & fill categorical before fit_transform\n",
    "        try:\n",
    "            preview_cols = num_present + cat_present\n",
    "            df_preview = df.loc[:, preview_cols].copy()\n",
    "            # numeric: coerce to numeric\n",
    "            for c in num_present:\n",
    "                if c in df_preview.columns:\n",
    "                    df_preview[c] = pd.to_numeric(df_preview[c], errors='coerce').fillna(df_preview[c].median())\n",
    "            # categorical: fillna with sentinel\n",
    "            for c in cat_present:\n",
    "                if c in df_preview.columns:\n",
    "                    df_preview[c] = df_preview[c].astype(str).fillna(\"__MISSING__\")\n",
    "            # fit_transform on a small sample for speed\n",
    "            sample_n = min(2000, max(100, int(len(df_preview) * 0.05)))\n",
    "            df_preview_sample = df_preview.sample(n=sample_n, random_state=42) if len(df_preview) > sample_n else df_preview\n",
    "            X_p = preprocess.fit_transform(df_preview_sample)\n",
    "            st.write(\"Preview transform shape:\", X_p.shape)\n",
    "        except Exception as e:\n",
    "            st.error(\"Preview transform failed â€” dataset may have incompatible dtypes or be empty for the selected columns.\")\n",
    "            if show_trace: st.text(traceback.format_exc())\n",
    "\n",
    "# -------------------------\n",
    "# TAB: Supervised\n",
    "# -------------------------\n",
    "with tabs[3]:\n",
    "    st.header(\"Supervised Models â€” Train & Evaluate (safe & visible)\")\n",
    "\n",
    "    # Choose target (preferred order)\n",
    "    if 'catdesc' in df.columns and df['catdesc'].nunique() > 1:\n",
    "        TARGET = 'catdesc'\n",
    "    elif 'catcode' in df.columns and df['catcode'].nunique() > 1:\n",
    "        TARGET = 'catcode'\n",
    "    elif 'units' in df.columns:\n",
    "        df['units_bin'] = pd.qcut(pd.to_numeric(df['units'].fillna(0)), q=4, labels=False, duplicates='drop')\n",
    "        TARGET = 'units_bin'\n",
    "        st.info(\"Using fallback target: units_bin (quartiles)\")\n",
    "    else:\n",
    "        TARGET = None\n",
    "        st.error(\"No suitable target found (catdesc/catcode/units).\")\n",
    "\n",
    "    if TARGET:\n",
    "        real_num = [c for c in NUM_COLS if c in df.columns]\n",
    "        real_cat = [c for c in CAT_COLS if c in df.columns]\n",
    "        FEATURES = real_cat + real_num\n",
    "        if not FEATURES:\n",
    "            st.error(\"No features available. Check NUM_COLS and CAT_COLS definitions.\")\n",
    "        else:\n",
    "            st.write(\"Features to be used:\", FEATURES)\n",
    "\n",
    "            # Choose sample vs full training\n",
    "            use_full_train = st.checkbox(\"Force full-data training (may be slow)\", value=train_full)\n",
    "            if use_full_train:\n",
    "                df_model = df.copy()\n",
    "            else:\n",
    "                df_model = df.sample(frac=sample_frac, random_state=42) if sample_mode else df.copy()\n",
    "                st.info(f\"Training on sample fraction {sample_frac:.2f} ({len(df_model)} rows).\")\n",
    "\n",
    "            # Prepare X and y safely\n",
    "            X = df_model[FEATURES].copy()\n",
    "            for c in real_num:\n",
    "                if c in X.columns:\n",
    "                    X[c] = pd.to_numeric(X[c], errors='coerce').fillna(X[c].median())\n",
    "            for c in real_cat:\n",
    "                if c in X.columns:\n",
    "                    X[c] = X[c].astype(str).fillna(\"__MISSING__\")\n",
    "\n",
    "            le_target = LabelEncoder()\n",
    "            y = le_target.fit_transform(df_model[TARGET].astype(str).fillna(\"__MISSING__\"))\n",
    "\n",
    "            vc = np.bincount(y)\n",
    "            strat = y if (len(vc) > 1 and vc.min() > 1) else None\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=strat)\n",
    "            st.write(\"Train/test shapes:\", X_train.shape, X_test.shape)\n",
    "\n",
    "            # Build preprocess for actual columns\n",
    "            transformers = []\n",
    "            if real_num:\n",
    "                transformers.append(('num', Pipeline([('impute', SimpleImputer(strategy='median')), ('scale', StandardScaler())]), real_num))\n",
    "            if real_cat:\n",
    "                transformers.append(('cat', Pipeline([('impute', SimpleImputer(strategy='most_frequent')), ('ohe', make_onehot())]), real_cat))\n",
    "            preprocess = ColumnTransformer(transformers) if transformers else None\n",
    "\n",
    "            # session state storage\n",
    "            if 'models' not in st.session_state:\n",
    "                st.session_state['models'] = {}\n",
    "            st.session_state['eval_data'] = {'X_test': X_test, 'y_test': y_test, 'label_encoder': le_target, 'features': FEATURES}\n",
    "\n",
    "            # Training buttons\n",
    "            col1, col2, col3 = st.columns(3)\n",
    "            with col1:\n",
    "                if st.button(\"Train Logistic Regression\"):\n",
    "                    try:\n",
    "                        pipe = Pipeline([('pre', preprocess), ('clf', LogisticRegression(max_iter=1000))])\n",
    "                        pipe.fit(X_train, y_train)\n",
    "                        st.session_state['models']['LR'] = pipe\n",
    "                        st.success(\"Logistic Regression trained.\")\n",
    "                    except Exception as e:\n",
    "                        st.error(\"Training LR failed: \" + str(e))\n",
    "                        if show_trace: st.text(traceback.format_exc())\n",
    "\n",
    "            with col2:\n",
    "                if st.button(\"Train KNN\"):\n",
    "                    try:\n",
    "                        pipe = Pipeline([('pre', preprocess), ('clf', KNeighborsClassifier(n_neighbors=5))])\n",
    "                        pipe.fit(X_train, y_train)\n",
    "                        st.session_state['models']['KNN'] = pipe\n",
    "                        st.success(\"KNN trained.\")\n",
    "                    except Exception as e:\n",
    "                        st.error(\"Training KNN failed: \" + str(e))\n",
    "                        if show_trace: st.text(traceback.format_exc())\n",
    "\n",
    "            with col3:\n",
    "                if st.button(\"Train Decision Tree\"):\n",
    "                    try:\n",
    "                        pipe = Pipeline([('pre', preprocess), ('clf', DecisionTreeClassifier(max_depth=6, random_state=42))])\n",
    "                        pipe.fit(X_train, y_train)\n",
    "                        st.session_state['models']['DT'] = pipe\n",
    "                        st.success(\"Decision Tree trained.\")\n",
    "                    except Exception as e:\n",
    "                        st.error(\"Training DT failed: \" + str(e))\n",
    "                        if show_trace: st.text(traceback.format_exc())\n",
    "\n",
    "            # Show summary if models exist + allow selection to evaluate\n",
    "            if st.session_state.get('models'):\n",
    "                st.subheader(\"Model accuracies (test set)\")\n",
    "                results = {}\n",
    "                for name, mdl in st.session_state['models'].items():\n",
    "                    try:\n",
    "                        preds = mdl.predict(X_test)\n",
    "                        results[name] = round(accuracy_score(y_test, preds), 4)\n",
    "                    except Exception as e:\n",
    "                        results[name] = f\"err: {e}\"\n",
    "                res_df = pd.DataFrame.from_dict(results, orient='index', columns=['accuracy'])\n",
    "                st.table(res_df)\n",
    "\n",
    "                # Evaluation dropdown\n",
    "                st.subheader(\"Detailed Evaluation\")\n",
    "                sel_model_name = st.selectbox(\"Choose model to inspect\", options=list(st.session_state['models'].keys()))\n",
    "                model = st.session_state['models'][sel_model_name]\n",
    "                try:\n",
    "                    y_pred = model.predict(X_test)\n",
    "                    # Confusion matrix\n",
    "                    cm = confusion_matrix(y_test, y_pred)\n",
    "                    fig, ax = plt.subplots(figsize=(6,4))\n",
    "                    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "                    ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"Actual\")\n",
    "                    st.pyplot(fig)\n",
    "\n",
    "                    # classification report (text)\n",
    "                    st.text(\"Classification report:\")\n",
    "                    st.text(classification_report(y_test, y_pred, target_names=list(le_target.classes_)))\n",
    "\n",
    "                    # precision/recall/fscore table + heatmap\n",
    "                    pr, rc, f1, sup = precision_recall_fscore_support(y_test, y_pred, average=None, zero_division=0)\n",
    "                    pr_df = pd.DataFrame({\n",
    "                        'precision': pr,\n",
    "                        'recall': rc,\n",
    "                        'f1': f1,\n",
    "                        'support': sup\n",
    "                    }, index=list(le_target.classes_))\n",
    "                    st.write(\"Per-class metrics:\")\n",
    "                    st.dataframe(pr_df.round(3))\n",
    "                    # heatmap\n",
    "                    fig2, ax2 = plt.subplots(figsize=(6,4))\n",
    "                    sns.heatmap(pr_df[['precision','recall','f1']], annot=True, fmt='.2f', cmap='viridis', ax=ax2)\n",
    "                    ax2.set_title(\"Per-class precision / recall / f1\")\n",
    "                    st.pyplot(fig2)\n",
    "\n",
    "                    # ROC plotting if predict_proba exists (binary or OVR multiclass)\n",
    "                    if hasattr(model, \"predict_proba\"):\n",
    "                        st.subheader(\"ROC / AUC (one-vs-rest for multiclass)\")\n",
    "                        from sklearn.preprocessing import label_binarize\n",
    "                        classes = np.arange(len(le_target.classes_))\n",
    "                        y_test_bin = label_binarize(y_test, classes=classes)\n",
    "                        probs = model.predict_proba(X_test)\n",
    "                        if y_test_bin.shape[1] == 1:\n",
    "                            fpr, tpr, _ = roc_curve(y_test, probs[:,1])\n",
    "                            roc_auc = auc(fpr, tpr)\n",
    "                            fig3, ax3 = plt.subplots()\n",
    "                            ax3.plot(fpr, tpr, label=f\"AUC={roc_auc:.3f}\")\n",
    "                            ax3.plot([0,1],[0,1], '--', color='gray')\n",
    "                            ax3.set_xlabel(\"FPR\"); ax3.set_ylabel(\"TPR\"); ax3.legend()\n",
    "                            st.pyplot(fig3)\n",
    "                        else:\n",
    "                            fig4, ax4 = plt.subplots(figsize=(6,4))\n",
    "                            for i in range(y_test_bin.shape[1]):\n",
    "                                fpr, tpr, _ = roc_curve(y_test_bin[:,i], probs[:,i])\n",
    "                                roc_auc = auc(fpr, tpr)\n",
    "                                ax4.plot(fpr, tpr, label=f\"{le_target.classes_[i]} (AUC={roc_auc:.2f})\")\n",
    "                            ax4.plot([0,1],[0,1],'--', color='gray')\n",
    "                            ax4.set_xlabel(\"FPR\"); ax4.set_ylabel(\"TPR\"); ax4.legend(bbox_to_anchor=(1.05,1))\n",
    "                            st.pyplot(fig4)\n",
    "                    else:\n",
    "                        st.info(\"Model has no predict_proba(); ROC cannot be plotted.\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    st.error(\"Evaluation failed: \" + str(e))\n",
    "                    if show_trace: st.text(traceback.format_exc())\n",
    "\n",
    "# -------------------------\n",
    "# TAB: Model Comparison\n",
    "# -------------------------\n",
    "with tabs[4]:\n",
    "    st.header(\"Model Comparison (visual)\")\n",
    "\n",
    "    if not st.session_state.get('models'):\n",
    "        st.info(\"No trained models to compare yet. Train models in the Supervised tab.\")\n",
    "    else:\n",
    "        eval_data = st.session_state.get('eval_data', {})\n",
    "        X_test = eval_data.get('X_test')\n",
    "        y_test = eval_data.get('y_test')\n",
    "        le_target = eval_data.get('label_encoder')\n",
    "\n",
    "        comp_rows = []\n",
    "        for name, mdl in st.session_state['models'].items():\n",
    "            try:\n",
    "                preds = mdl.predict(X_test)\n",
    "                acc = accuracy_score(y_test, preds)\n",
    "                comp_rows.append({'model': name, 'accuracy': acc})\n",
    "            except Exception:\n",
    "                comp_rows.append({'model': name, 'accuracy': np.nan})\n",
    "        comp_df = pd.DataFrame(comp_rows).set_index('model')\n",
    "        st.table(comp_df)\n",
    "\n",
    "        # bar chart\n",
    "        if not comp_df['accuracy'].isna().all():\n",
    "            fig = px.bar(comp_df.reset_index(), x='model', y='accuracy', title='Model accuracy comparison', text='accuracy')\n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "\n",
    "        # show aggregated per-class metrics heatmap across models (if feasible)\n",
    "        st.subheader(\"Per-class F1 heatmap across models\")\n",
    "        try:\n",
    "            f1_table = {}\n",
    "            for name, mdl in st.session_state['models'].items():\n",
    "                preds = mdl.predict(X_test)\n",
    "                _, _, f1, _ = precision_recall_fscore_support(y_test, preds, average=None, zero_division=0)\n",
    "                f1_table[name] = f1\n",
    "            f1_df = pd.DataFrame(f1_table, index=list(le_target.classes_))\n",
    "            fig, ax = plt.subplots(figsize=(8, max(4, 0.5 * len(f1_df))))\n",
    "            sns.heatmap(f1_df, annot=True, fmt='.2f', cmap='magma', ax=ax)\n",
    "            ax.set_ylabel(\"Class\"); ax.set_xlabel(\"Model\")\n",
    "            st.pyplot(fig)\n",
    "        except Exception as e:\n",
    "            st.info(\"Per-class heatmap skipped: \" + str(e))\n",
    "            if show_trace: st.text(traceback.format_exc())\n",
    "\n",
    "# -------------------------\n",
    "# TAB: Predict (auto for all models)\n",
    "# -------------------------\n",
    "with tabs[5]:\n",
    "    st.header(\"Predict â€” Run predictions for all trained models (auto)\")\n",
    "\n",
    "    if not st.session_state.get('models'):\n",
    "        st.info(\"No trained supervised models available. Train them in the Supervised tab.\")\n",
    "    else:\n",
    "        models = st.session_state['models']\n",
    "        # Determine features for prediction â€” best-effort from first model's preprocess\n",
    "        any_model = next(iter(models.values()))\n",
    "        features_for_pred = None\n",
    "        try:\n",
    "            pre = any_model.named_steps.get('pre')\n",
    "            if pre is not None and hasattr(pre, 'transformers'):\n",
    "                features_for_pred = []\n",
    "                for name, trans, cols in pre.transformers:\n",
    "                    if isinstance(cols, (list, tuple)):\n",
    "                        features_for_pred.extend(cols)\n",
    "        except Exception:\n",
    "            features_for_pred = None\n",
    "\n",
    "        if not features_for_pred:\n",
    "            features_for_pred = [c for c in NUM_COLS if c in df.columns]\n",
    "\n",
    "        st.write(\"Features used for prediction (inferred):\", features_for_pred)\n",
    "\n",
    "        # Prepare X_pred safely\n",
    "        X_pred = df[features_for_pred].copy()\n",
    "        for c in X_pred.columns:\n",
    "            if c in NUM_COLS:\n",
    "                X_pred[c] = pd.to_numeric(X_pred[c], errors='coerce').fillna(X_pred[c].median())\n",
    "            else:\n",
    "                X_pred[c] = X_pred[c].astype(str).fillna(\"__MISSING__\")\n",
    "\n",
    "        # Build panels for each model: show head and download\n",
    "        cols = st.columns(len(models))\n",
    "        for col, (name, mdl) in zip(cols, models.items()):\n",
    "            with col:\n",
    "                st.subheader(name)\n",
    "                try:\n",
    "                    preds = mdl.predict(X_pred)\n",
    "                    df_out = df.copy()\n",
    "                    df_out['prediction_' + name] = preds\n",
    "                    st.dataframe(df_out[[*features_for_pred][:3] + ['prediction_' + name]].head(10))\n",
    "                    csv_bytes = df_out.to_csv(index=False).encode('utf-8')\n",
    "                    st.download_button(f\"Download predictions ({name})\", data=csv_bytes, file_name=f\"predictions_{name}.csv\", mime=\"text/csv\")\n",
    "                    st.write(\"Unique prediction counts:\", pd.Series(preds).value_counts().to_dict())\n",
    "                except Exception as e:\n",
    "                    st.error(f\"Prediction failed for {name}: {e}\")\n",
    "                    if show_trace: st.text(traceback.format_exc())\n",
    "\n",
    "# -------------------------\n",
    "# TAB: Unsupervised\n",
    "# -------------------------\n",
    "with tabs[6]:\n",
    "    st.header(\"Unsupervised â€” KMeans, Agglomerative, PCA, Pairplots, Dendrogram\")\n",
    "\n",
    "    real_num = [c for c in NUM_COLS if c in df.columns]\n",
    "    if not real_num:\n",
    "        st.error(\"No numeric columns found for unsupervised analysis (e.g., 'units','load').\")\n",
    "    else:\n",
    "        st.write(\"Numeric features used:\", real_num)\n",
    "\n",
    "        X_unsup = df[real_num].copy()\n",
    "        for c in real_num:\n",
    "            X_unsup[c] = pd.to_numeric(X_unsup[c], errors='coerce').fillna(X_unsup[c].median())\n",
    "\n",
    "        # Elbow\n",
    "        st.subheader(\"Elbow method (inertia vs k)\")\n",
    "        K_range = list(range(2, min(10, max(3, len(df)//10)) + 1))\n",
    "        inertias = []\n",
    "        for k in K_range:\n",
    "            try:\n",
    "                km_tmp = KMeans(n_clusters=k, random_state=42, n_init=5)\n",
    "                km_tmp.fit(X_unsup)\n",
    "                inertias.append(km_tmp.inertia_)\n",
    "            except Exception:\n",
    "                inertias.append(np.nan)\n",
    "        fig, ax = plt.subplots(figsize=(6,3))\n",
    "        ax.plot(K_range, inertias, marker='o', color=\"#0f4c81\")\n",
    "        ax.set_xlabel(\"k\"); ax.set_ylabel(\"Inertia\"); ax.set_title(\"Elbow\")\n",
    "        st.pyplot(fig)\n",
    "\n",
    "        k = st.slider(\"Choose k for clustering\", 2, min(12, max(2, len(df)//10)), 4)\n",
    "\n",
    "        # KMeans\n",
    "        st.subheader(\"KMeans clustering & PCA\")\n",
    "        try:\n",
    "            km = KMeans(n_clusters=k, random_state=42, n_init=5)\n",
    "            labels = km.fit_predict(X_unsup)\n",
    "            st.write(\"Cluster counts (KMeans):\", pd.Series(labels).value_counts().to_dict())\n",
    "\n",
    "            pca = PCA(n_components=2)\n",
    "            proj = pca.fit_transform(X_unsup)\n",
    "            plot_df = pd.DataFrame({'PCA1': proj[:,0], 'PCA2': proj[:,1], 'cluster': labels.astype(str)})\n",
    "            fig = px.scatter(plot_df, x='PCA1', y='PCA2', color='cluster', title=f'KMeans k={k} (PCA)')\n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "\n",
    "            # attach cluster column to main df for pairplot usage\n",
    "            df['_KM_Cluster'] = labels\n",
    "        except Exception as e:\n",
    "            st.error(\"KMeans failed: \" + str(e))\n",
    "            if show_trace: st.text(traceback.format_exc())\n",
    "\n",
    "        # Agglomerative\n",
    "        st.subheader(\"Agglomerative / Hierarchical clustering\")\n",
    "        try:\n",
    "            agg = AgglomerativeClustering(n_clusters=k, linkage='ward')\n",
    "            labels_agg = agg.fit_predict(X_unsup)\n",
    "            st.write(\"Cluster counts (Agglomerative):\", pd.Series(labels_agg).value_counts().to_dict())\n",
    "\n",
    "            proj_agg = pca.transform(X_unsup)\n",
    "            plot_df2 = pd.DataFrame({'PCA1': proj_agg[:,0], 'PCA2': proj_agg[:,1], 'cluster': labels_agg.astype(str)})\n",
    "            fig2 = px.scatter(plot_df2, x='PCA1', y='PCA2', color='cluster', title=f'Agglomerative k={k} (PCA)')\n",
    "            st.plotly_chart(fig2, use_container_width=True)\n",
    "\n",
    "            df['_AGG_Cluster'] = labels_agg\n",
    "        except Exception as e:\n",
    "            st.error(\"Agglomerative failed: \" + str(e))\n",
    "            if show_trace: st.text(traceback.format_exc())\n",
    "\n",
    "        # Pairplots (your requested sns.pairplot lines) â€” do it safely (sampling + limited features)\n",
    "        st.subheader(\"Pairplots of clusters (sampled & limited columns)\")\n",
    "        try:\n",
    "            # choose up to 6 numeric features to keep pairplot reasonable\n",
    "            unsup_features = real_num[:6]  # at most 6\n",
    "            sample_n = min(800, max(100, int(len(df) * 0.1)))  # sample up to 800 or 10% of data\n",
    "            df_pair = df[[*unsup_features]].copy().sample(n=sample_n, random_state=42) if len(df) > sample_n else df[[*unsup_features]].copy()\n",
    "            # add cluster columns if present\n",
    "            if '_KM_Cluster' in df.columns:\n",
    "                df_pair['_KM_Cluster'] = df.loc[df_pair.index, '_KM_Cluster']\n",
    "                st.write(\"Pairplot for KMeans clusters (may take a moment)...\")\n",
    "                g = sns.pairplot(df_pair, vars=unsup_features, hue=\"_KM_Cluster\", palette=\"tab10\", diag_kind=\"kde\", plot_kws={'s':15})\n",
    "                st.pyplot(g.fig)\n",
    "                plt.close(g.fig)\n",
    "            else:\n",
    "                st.info(\"KMeans cluster column missing â€” run KMeans above to create it.\")\n",
    "\n",
    "            if '_AGG_Cluster' in df.columns:\n",
    "                df_pair2 = df.loc[df_pair.index, [*unsup_features]].copy()\n",
    "                df_pair2['_AGG_Cluster'] = df.loc[df_pair2.index, '_AGG_Cluster']\n",
    "                st.write(\"Pairplot for Agglomerative clusters (may take a moment)...\")\n",
    "                g2 = sns.pairplot(df_pair2, vars=unsup_features, hue=\"_AGG_Cluster\", palette=\"Set2\", diag_kind=\"kde\", plot_kws={'s':15})\n",
    "                st.pyplot(g2.fig)\n",
    "                plt.close(g2.fig)\n",
    "            else:\n",
    "                st.info(\"Agglomerative cluster column missing â€” run Agglomerative above to create it.\")\n",
    "        except Exception as e:\n",
    "            st.error(\"Pairplot generation failed or is too slow for full data: \" + str(e))\n",
    "            if show_trace: st.text(traceback.format_exc())\n",
    "\n",
    "        # Dendrogram (sampled)\n",
    "        st.subheader(\"Dendrogram (sampled)\")\n",
    "        try:\n",
    "            sample_n2 = min(200, len(X_unsup))\n",
    "            Z = linkage(X_unsup.sample(sample_n2, random_state=42), method='ward')\n",
    "            fig, ax = plt.subplots(figsize=(10,4))\n",
    "            dendrogram(Z, truncate_mode='lastp', p=12, ax=ax)\n",
    "            ax.set_title(\"Hierarchical Clustering Dendrogram (sampled)\")\n",
    "            st.pyplot(fig)\n",
    "        except Exception as e:\n",
    "            st.info(\"Dendrogram skipped/failed: \" + str(e))\n",
    "            if show_trace: st.text(traceback.format_exc())\n",
    "\n",
    "# -------------------------\n",
    "# TAB: Save models\n",
    "# -------------------------\n",
    "with tabs[7]:\n",
    "    st.header(\"Save / Download Trained Models\")\n",
    "    if not st.session_state.get('models'):\n",
    "        st.info(\"No trained models in session. Train models in the Supervised tab.\")\n",
    "    else:\n",
    "        for name, mdl in st.session_state['models'].items():\n",
    "            st.write(\"Model:\", name)\n",
    "            bio = download_bytesio(mdl)\n",
    "            st.download_button(f\"Download {name}.pkl\", data=bio, file_name=f\"{name}.pkl\")\n",
    "\n",
    "st.markdown(\"<div style='margin-top:14px; color:#6b7280'>Dashboard V2 ready â€” metrics visible, comparison added, pairplots included (sampled & safe). WordCloud now built from multiple columns.</div>\", unsafe_allow_html=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
